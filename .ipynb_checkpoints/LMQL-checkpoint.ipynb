{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e378de-974f-4740-bf9f-f817fa99d94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892404c-b339-4845-be72-d00e6fc69071",
   "metadata": {},
   "source": [
    "# Simple token constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918c2f16-c3e6-42ac-bdb0-519c5a5e17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def tipo_test():\n",
    "    '''lmql\n",
    "    argmax\n",
    "        \"Pregunta: Sevilla es una ciudad ubicada en que parte de Espana \\n\"\n",
    "        \"A) Sur \\n\"\n",
    "        \"B) Norte \\n\"\n",
    "        \"C) Este \\n\"\n",
    "        \"D) Afuera de Espana \\n\"\n",
    "        \"Respuesta: [RESPUESTA]\"\n",
    "    from\n",
    "        \"openai/text-davinci-003\"\n",
    "    where\n",
    "        RESPUESTA in set([\"A\", \"B\", \"C\", \"D\"])\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7692251-9a00-4ae3-8a3f-d75472c3b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await tipo_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b024e9-3eac-4056-9db9-61a1fadd5a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LMQLResult(prompt='Pregunta: Sevilla es una ciudad ubicada en que parte de Espana \\nA) Sur \\nB) Norte \\nC) Este \\nD) Afuera de Espana \\nRespuesta: A', variables={'RESPUESTA': 'A'}, distribution_variable=None, distribution_values=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae22464-6471-4101-bfce-bf55b2a16390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RESPUESTA': 'A'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c1e27-61a6-4844-8133-07550138c8fa",
   "metadata": {},
   "source": [
    "# Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b82df9-ff42-4562-99b1-1388a7686c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def sentiment_analysis(input):\n",
    "    '''lmql\n",
    "    argmax\n",
    "        \"Classifica el sentimiento de la siguiente frase como POS, NEG O NEUT: \\n\"\n",
    "        \"{input} \\n\"\n",
    "        \"El sentimento de la frase es [SENTIMENT]\"\n",
    "    from\n",
    "        \"openai/text-davinci-003\"\n",
    "    distribution\n",
    "        SENTIMENT in [\"POS\", \"NEUT\", \"NEG\"]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95768eb-396a-47a1-b2dd-c3364b6b51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiashaugestad/opt/anaconda3/envs/semantic-kernel/lib/python3.11/site-packages/lmql/runtime/caching.py:33: UserWarning: LMQL cache directory (/Users/mathiashaugestad/.cache/lmql) format is outdated, clearing cache (existing: v3, runtime: v5)...\n",
      "  warnings.warn(\"LMQL cache directory ({}) format is outdated, clearing cache (existing: v{}, runtime: v{})...\".format(CACHE_DIR, cache_version, CACHE_VERSION))\n",
      "/Users/mathiashaugestad/opt/anaconda3/envs/semantic-kernel/lib/python3.11/site-packages/lmql/runtime/openai_integration.py:237: UserWarning: warning: The OpenAI API has merged 1 token(s) server-side, which will reflect in inaccurate 0.0 scores in the decoding tree\n",
      "  warnings.warn(\"warning: The OpenAI API has merged {} token(s) server-side, which will reflect in inaccurate 0.0 scores in the decoding tree\".format(server_side_swallowed_tokens))\n"
     ]
    }
   ],
   "source": [
    "r = await sentiment_analysis('Estoy muy feliz hoy!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6664e56e-cbec-4ae8-b51b-6aa6ddd97b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SENTIMENT', 'P(SENTIMENT)', 'log P(SENTIMENT)'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06690939-da34-4d3e-a70c-94828a41a6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('POS', 0.3336381603931782),\n",
       " ('NEUT', 0.33275111579463185),\n",
       " ('NEG', 0.33361072381218987)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.variables.get('P(SENTIMENT)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b8103-b14d-42f2-9315-1c95db76958b",
   "metadata": {},
   "source": [
    "# JSON response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d65e7bf-15e5-46a2-82ad-53b5c40f60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f478fdc6-974a-4c05-9227-c2f93da61d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only works in playground :p ?? \n",
    "# This only works if install directly from github, not pip (ie pip install git+https://github.com/eth-sri/lmql)\n",
    "\n",
    "\n",
    "@lmql.query\n",
    "async def jsonize():\n",
    "    '''lmql\n",
    "        from dataclasses import dataclass\n",
    "        import lmql\n",
    "        \n",
    "        @dataclass\n",
    "        class Event:\n",
    "            nombre: str\n",
    "            descripcion: str\n",
    "                \n",
    "        @dataclass\n",
    "        class Meetup:\n",
    "            nombre: str\n",
    "            organizadores: str\n",
    "            event: Event\n",
    "    \n",
    "        argmax\n",
    "            \"\"\"\n",
    "            Por favor, coloca el texto a continuación en un formato JSON estructurado:\n",
    "            IA Generativa Sevilla es un grupo de meetup colocada en Sevilla, Espana. Los organizadores son Ivan y Mathias\n",
    "            El evento de hoy trata de dominando un LLM utilizando LMQL\n",
    "            structured: [MEETUP]\n",
    "            \"\"\"\n",
    "        from\n",
    "            \"openai/text-davinci-003\"\n",
    "        where\n",
    "            type(MEETUP) is Meetup\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5588ec17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMQLResult(prompt=\"\\nPor favor, coloca el texto a continuación en un formato JSON estructurado:\\nIA Generativa Sevilla es un grupo de meetup colocada en Sevilla, Espana. Los organizadores son Ivan y Mathias\\nEl evento de hoy trata de dominando un LLM utilizando LMQL\\nstructured: Meetup(nombre='IA Generativa Sevilla', organizadores='Ivan y Mathias', event=Event(nombre='Dominando un LLM utilizando LMQL', descripcion='Un grupo de meetup en Sevilla, Espana'))\\n\", variables={'MEETUP': Meetup(nombre='IA Generativa Sevilla', organizadores='Ivan y Mathias', event=Event(nombre='Dominando un LLM utilizando LMQL', descripcion='Un grupo de meetup en Sevilla, Espana'))}, distribution_variable=None, distribution_values=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await jsonize()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d459fb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meetup(nombre='IA Generativa Sevilla', organizadores='Ivan y Mathias', event=Event(nombre='Dominando un LLM utilizando LMQL', descripcion='Un grupo de meetup en Sevilla, Espana'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.variables['MEETUP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a859ae6",
   "metadata": {},
   "source": [
    "# Create json without dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed37483-7d2c-439c-801e-fc16224f73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def jsonize():\n",
    "    '''lmql\n",
    "    argmax \n",
    "        \"\"\"\n",
    "        Por favor, coloca el texto a continuación en un formato JSON estructurado:\n",
    "\n",
    "        IA Generativa Sevilla es un grupo de meetup colocada en Sevilla, Espana. Los organizadores son Ivan y Mathias\n",
    "        El evento de hoy trata de dominando un LLM utilizando LMQL\n",
    "\n",
    "        {{\n",
    "        \"meetup-name\": \"[NAME]\",\n",
    "        \"location\": \"[LOCATION]\",\n",
    "        \"organizers\": \"[ORGANIZERS]\",\n",
    "        \"description\": \"[DESCRIPTION]\",\n",
    "        }}\n",
    "        \"\"\"\n",
    "    from\n",
    "        \"openai/text-davinci-003\" \n",
    "    where\n",
    "        STOPS_BEFORE(NAME, '\"') and STOPS_BEFORE(DESCRIPTION, '\"') and STOPS_BEFORE(LOCATION, '\"') and STOPS_BEFORE(ORGANIZERS, '\"') and len(NAME) < 25\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a73c3478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiashaugestad/opt/anaconda3/envs/semantic-kernel/lib/python3.11/site-packages/lmql/runtime/bopenai/batched_openai.py:741: OpenAILogitBiasLimitationWarning: the required logit_bias is too large to be handled by the OpenAI API and will be limited to the first 300 tokens. This can lead to the violation of the provided constraints or undesired model output. To avoid this use less broad or no constraints.\n",
      "  warnings.warn(\"the required logit_bias is too large to be handled by the OpenAI API and will be limited to the first 300 tokens. This can lead to the violation of the provided constraints or undesired model output. To avoid this use less broad or no constraints.\", category=OpenAILogitBiasLimitationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LMQLResult(prompt='\\nPor favor, coloca el texto a continuación en un formato JSON estructurado:\\nIA Generativa Sevilla es un grupo de meetup colocada en Sevilla, Espana. Los organizadores son Ivan y Mathias\\nEl evento de hoy trata de dominando un LLM utilizando LMQL\\n{\\n\"meetup-name\": \"IA Generativa Sevilla\",\\n\"location\": \"Sevilla, Espana\",\\n\"organizers\": \"Ivan y Mathias\",\\n\"description\": \"Dominando un LLM utilizando LMQL\",\\n}\\n', variables={'NAME': 'IA Generativa Sevilla', 'LOCATION': 'Sevilla, Espana', 'ORGANIZERS': 'Ivan y Mathias', 'DESCRIPTION': 'Dominando un LLM utilizando LMQL'}, distribution_variable=None, distribution_values=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await jsonize()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c98c3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAME': 'IA Generativa Sevilla',\n",
       " 'LOCATION': 'Sevilla, Espana',\n",
       " 'ORGANIZERS': 'Ivan y Mathias',\n",
       " 'DESCRIPTION': 'Dominando un LLM utilizando LMQL'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68d415",
   "metadata": {},
   "source": [
    "# Calling function / control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88691af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f217dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.dictionaryapi.dev/api/v2/entries/en/{word}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2620d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_definitions(word):\n",
    "    data = requests.get(url.format(word='asynchronous')).json()\n",
    "    # Extract the definitions\n",
    "    definitions = []\n",
    "    for meaning in data[0]['meanings'][0]['definitions']:\n",
    "        definitions.append(f\"definition: {meaning['definition']}\")\n",
    "    return definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "98b1f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def example_sentence(term):\n",
    "    '''lmql\n",
    "    argmax\n",
    "        \"\"\"\n",
    "        Please provide an example sentences for each definition of the word: {term}\n",
    "        \"\"\"\n",
    "\n",
    "        definitions = await get_definitions(term)\n",
    "        definition_strings = \"\\n\".join(definitions)\n",
    "\n",
    "        \"\"\"\n",
    "        Here are the dictionary definitions:\n",
    "        {definition_strings}\n",
    "        \"\"\"\n",
    "        \n",
    "        examples = []\n",
    "        for i in range(len(definitions)):\n",
    "            \"-[EXAMPLE]\" where STOPS_AT(EXAMPLE, \"\\n\")\n",
    "            examples.append(EXAMPLE.strip())\n",
    "    from\n",
    "        \"openai/text-davinci-003\"\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b874fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await example_sentence(\"Asynchronous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3dd6d-4a18-4e0f-b19c-6ab719593477",
   "metadata": {},
   "source": [
    "# Open Source Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9174c1-e327-4825-8237-146adf9ed631",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def example_os_model_sql(pregunta):\n",
    "        '''lmql\n",
    "        argmax \n",
    "            \"\"\"\n",
    "            Dada la tabla a continuación, por favor responda la pregunta a continuación con una consulta SQL válida.\n",
    "            Por favor, utilice solamente la tabla que se encuentra a continuación:\n",
    "\n",
    "            ### Tablas:\n",
    "            CREATE TABLE transactions (\n",
    "                transaction_id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "                customer_id INT NOT NULL,\n",
    "                product_id INT NOT NULL,\n",
    "                transaction_date DATE NOT NULL,\n",
    "                amount DECIMAL(10, 2) NOT NULL,\n",
    "                payment_method VARCHAR(50) NOT NULL,\n",
    "                transaction_status VARCHAR(20) NOT NULL,\n",
    "                FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n",
    "                FOREIGN KEY (product_id) REFERENCES products(product_id)\n",
    "            );\n",
    "    \n",
    "            Q: {pregunta}\n",
    "            A:[QUERY]\n",
    "            \"\"\"\n",
    "        from\n",
    "            lmql.model(\"local:codellama/CodeLlama-7b-Instruct-hf\", cuda=True)\n",
    "        where\n",
    "            STOPS_BEFORE(QUERY, ';')\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eabc09-3a0b-45a1-9116-45b19d11ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tambien se puede crear un server local: este debe ser mas rapido\n",
    "# https://docs.lmql.ai/en/stable/language/hf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f293a7c-5338-4a5d-81fa-9990695aa6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading codellama/CodeLlama-7b-Instruct-hf with AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\", device_map=auto)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread scheduler-worker:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lmql/models/lmtp/lmtp_scheduler.py\", line 248, in worker\n",
      "    model = LMTPModel.load(self.model_identifier, **self.model_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lmql/models/lmtp/backends/lmtp_model.py\", line 51, in load\n",
      "    return LMTPModel.registry[\"transformers\"](model_name, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/lmql/models/lmtp/backends/transformers_model.py\", line 31, in __init__\n",
      "    self.model = AutoModelForCausalLM.from_pretrained(self.model_identifier, **self.model_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 565, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2581, in from_pretrained\n",
      "    raise ImportError(\n",
      "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/models/lmtp/lmtp_dcmodel.py:138\u001b[0m, in \u001b[0;36mLMTPDcModel.stream_and_return_first\u001b[0;34m(self, s, iterator, sampling_mode)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     buffer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m anext(iterator)]\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/models/lmtp/lmtp_multiprocessing.py:158\u001b[0m, in \u001b[0;36mLMTPMultiProcessingClient.generate\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGENERATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, payload))\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_id):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/models/lmtp/lmtp_multiprocessing.py:181\u001b[0m, in \u001b[0;36mLMTPMultiProcessingClient.stream_iterator\u001b[0;34m(self, stream_id)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m q\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/queues.py:159\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m getter\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m example_os_model_sql(pregunta\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPor favor, proporciona las últimas cinco transacciones.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/runtime/lmql_runtime.py:230\u001b[0m, in \u001b[0;36mLMQLQueryFunction.__acall__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         PromptInterpreter\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m=\u001b[39m interpreter\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# execute main prompt\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfct, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquery_kwargs)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PromptInterpreter\u001b[38;5;241m.\u001b[39mmain \u001b[38;5;241m==\u001b[39m interpreter:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/runtime/interpreter.py:1035\u001b[0m, in \u001b[0;36mPromptInterpreter.run\u001b[0;34m(self, fct, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m average_step_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1035\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m decoder_fct(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoder_args):\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m debug_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_step)\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/runtime/dclib/decoders.py:21\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(prompt_ids, n, max_len, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m h\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(h) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39margmax(h, noscore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     22\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mrewrite(h, noscore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     h, done \u001b[38;5;241m=\u001b[39m (h \u001b[38;5;241m+\u001b[39m done)\u001b[38;5;241m.\u001b[39mseparate_by(dc\u001b[38;5;241m.\u001b[39mlogical_not(dc\u001b[38;5;241m.\u001b[39meos), dc\u001b[38;5;241m.\u001b[39mlt(max_len))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/runtime/dclib/dclib_cache.py:277\u001b[0m, in \u001b[0;36mCachedDcModel.argmax\u001b[0;34m(self, arr, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# read full result from cache\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m arr\u001b[38;5;241m.\u001b[39maelement_wise(op_argmax)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lmql/runtime/dclib/dclib_array.py:318\u001b[0m, in \u001b[0;36mDataArray.aelement_wise\u001b[0;34m(self, op, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop_with_path\u001b[39m(path, element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mawait\u001b[39;00m op(element, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 318\u001b[0m result_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[op_with_path(path, seqs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m path, seqs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataArray(\u001b[38;5;28mdict\u001b[39m(result_items))\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resp = await example_os_model_sql(pregunta=\"Por favor, proporciona las últimas cinco transacciones.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c15423-d761-42b1-9f5b-f30693b78c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAnswer:\\n\\n\\\\begin{code}\\nSELECT * FROM transactions ORDER BY transaction_date DESC LIMIT 5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.variables.get('QUERY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
